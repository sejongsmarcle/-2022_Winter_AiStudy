{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95e1bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 구동하는 데 필요한 kearas 함수 호출\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# 필요한 라이브러리 불러옴\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# 준비된 수술 환자 데이터를 불러옴\n",
    "Data_set = np.loadtxt(\"../../dataset/ThoraricSurgery.csv\",delimiter=\",\") # 구분인자: , <- 없으면 파일 인식 못 함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b509c6",
   "metadata": {},
   "source": [
    "# 데이터의 구분\n",
    "| | 정보 1 | 정보 2 | 정보 3 | ... | 정보 17 | 생존 여부 |\n",
    "|---|---|---|---|---|---|---|\n",
    "| 환자1 | | | | | | 0 |\n",
    "| 환자2 | | | | | | 0 |\n",
    "| 환자3 | | | | | | 1 |\n",
    "| ... | | | | | |  |\n",
    "| 환자470 | | | | | | 1 |\n",
    "\n",
    "데이터셋은 현재 위의 표와 같다. 이때 데이터를 속성(feature), 클래스, 샘플로 구분할 수 있다.\n",
    "* 속성(attribute)/feature: 정보 1~정보 17에 해당하는 부분\n",
    "* 샘플/instance: 환자1~환자470에 해당하는 부분\n",
    "* 클래스(class): 생존 여부에 해당하는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05282d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환자의 기록과 수술 결과를 X와 Y로 구분하여 저장\n",
    "X=Data_set[:,:17] # 모든 행에서 0~17 열 가져 옴 -> 470 x 17\n",
    "Y=Data_set[:,17] # 모든 행에서 17번 열 가져 옴 -> 470 x 1\n",
    "### cf. 리스트 슬라이싱 (https://m.blog.naver.com/jjys9047/221566766122)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "856dbd80",
   "metadata": {},
   "source": [
    "# 1. 딥러닝 구조 결정: 입력층, 은닉층, 출력층\n",
    "``` python\n",
    "model.add(Dense(30, input_dim=17, activation='relu'))\n",
    "```\n",
    "* add(): 새로운 층(layer) 추가\n",
    "* Dense(): 이 층에 몇 개의 노드 만들 것인지\n",
    "* input_dim: 입력 데이터에서 몇 개의 값을 가져올지\n",
    "kears는 입력층을 따로 만드는 것이 아니라, 첫 번째 은닉층에 input_dim을 적어줘서 첫 번째 Dense가 은닉층 + 입력층 역할을 하게 한다.\n",
    "\n",
    "위의 코드는 데이터에서 17개의 값을 입력으로 받아 은닉층의 30개 노드로 전달한다는 의미이고 그림으로 표현하면 아래와 같다.\n",
    "\n",
    "\n",
    "![은닉층 Dense](https://user-images.githubusercontent.com/78032658/150663492-0c37db84-8510-4716-905d-a95858410860.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0d48420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 구조를 결정(모델을 설정하고 실행하는 부분)\n",
    "model = Sequential()\n",
    "### Sequential 모델: 각 레이어에 정확히 하나의 입력 텐서와 하나의 출력 텐서가 있는 일반 레이어 스택에 적합한 TensorFlow 모델\n",
    "### cf. Sequential 모델 소개 (https://www.tensorflow.org/guide/keras/sequential_model?hl=ko)\n",
    "model.add(Dense(30, input_dim=17, activation='relu')) # 층 +1 -> 은닉층, Dense\n",
    "model.add(Dense(1,activation='sigmoid')) # 층 +1 -> 출력층(출력 값을 하나로 정해서 보여 줘야 하므로 노드 1개)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64432c7",
   "metadata": {},
   "source": [
    "17개의 입력 값은 은닉층의 각 노드로 들어가서 임의의 가중치를 갖고 활성화 함수에 들어가게 된다. 그리고 활성화 함수를 거친 결괏값이 출력층으로 전달된다.   \n",
    "Ex. 아래의 수식은 입력 값이 2개이고, 은닉층의 노드가 2개일 때 하나의 출력 값 y의 계산이다.\n",
    "\n",
    "\n",
    "$$ n_{1} = \\sigma(x_{1}w_{11}+x_{2}w_{21}+b_{1}) $$\n",
    "$$ n_{2} = \\sigma(x_{1}w_{12}+x_{2}w_{22}+b_{2}) $$\n",
    "$$ y_{out} = \\sigma(n_{1}w_{31}+n_{2}w_{32}+b_{3}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed246e",
   "metadata": {},
   "source": [
    "# 2. 모델 컴파일 \n",
    "모델을 컴파일 한다. 앞서 지정한 모델의 구현을 위해서 여러 가지 환경을 결정 및 설정하여 컴파일하는 부분이다.   \n",
    "1. 오차 함수 -> 평균 제곱 오차 함수(mean_squared_error)\n",
    "2. optimizer(최적화 방법) -> adam\n",
    "3. metrics(): 모델이 컴파일될 때 모델 수행 결과를 나타내도록 설정하는 부분. 테스트 샘플을 학습 과정에서 제외시켜서 overfitting 방지   \n",
    "(테스트 샘플은 정확도를 측정하기 위해 사용된다.)\n",
    "\n",
    "![폐암 환자 생존율 예측 신경망 모델 도식화](https://user-images.githubusercontent.com/78032658/150664470-8d036bd7-0974-42d3-bb19-14f50090fe71.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55cf0b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 실행\n",
    "model.compile(loss='mean_squared_error',optimzer='adam',metrics=['accuracy'])\n",
    "### 위에서 정해진 모델을 컴퓨터가 알아들을 수 있게끔 컴파일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd8368",
   "metadata": {},
   "source": [
    "# 3. 딥러닝 모델 실행\n",
    "앞서 컴파일 단계에서 설정한 환경에서 데이터를 불러와 딥러닝을 실행시킨다.  \n",
    "* epochs: 각 샘플이 해당 수 만큼 재사용될 때까지 반복해라.\n",
    "* batch_size: 한 번에 처리하는 샘플의 개수.너무 크면 학습 속도가 느려지고, 너무 작으면 각 실행 값의 편차가 생겨서 전체 결괏값이 불안정해짐. -> 자신의 컴퓨터 메모리에 적당한 batch_size 찾아 설정해주어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd2a200c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 470 samples\n",
      "Epoch 1/100\n",
      "470/470 [==============================] - 0s 671us/sample - loss: 0.1472 - accuracy: 0.8447\n",
      "Epoch 2/100\n",
      "470/470 [==============================] - 0s 66us/sample - loss: 0.1478 - accuracy: 0.8340\n",
      "Epoch 3/100\n",
      "470/470 [==============================] - 0s 68us/sample - loss: 0.1401 - accuracy: 0.8426\n",
      "Epoch 4/100\n",
      "470/470 [==============================] - 0s 68us/sample - loss: 0.1384 - accuracy: 0.8489\n",
      "Epoch 5/100\n",
      "470/470 [==============================] - 0s 74us/sample - loss: 0.1408 - accuracy: 0.8340\n",
      "Epoch 6/100\n",
      "470/470 [==============================] - 0s 76us/sample - loss: 0.1363 - accuracy: 0.8468\n",
      "Epoch 7/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1417 - accuracy: 0.8383\n",
      "Epoch 8/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1471 - accuracy: 0.8234\n",
      "Epoch 9/100\n",
      "470/470 [==============================] - 0s 70us/sample - loss: 0.1343 - accuracy: 0.8489\n",
      "Epoch 10/100\n",
      "470/470 [==============================] - 0s 71us/sample - loss: 0.1407 - accuracy: 0.8383\n",
      "Epoch 11/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1376 - accuracy: 0.8426\n",
      "Epoch 12/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1349 - accuracy: 0.8489\n",
      "Epoch 13/100\n",
      "470/470 [==============================] - 0s 73us/sample - loss: 0.1405 - accuracy: 0.8426\n",
      "Epoch 14/100\n",
      "470/470 [==============================] - 0s 71us/sample - loss: 0.1429 - accuracy: 0.8277\n",
      "Epoch 15/100\n",
      "470/470 [==============================] - 0s 72us/sample - loss: 0.1400 - accuracy: 0.8404\n",
      "Epoch 16/100\n",
      "470/470 [==============================] - 0s 73us/sample - loss: 0.1344 - accuracy: 0.8447\n",
      "Epoch 17/100\n",
      "470/470 [==============================] - 0s 75us/sample - loss: 0.1384 - accuracy: 0.8511\n",
      "Epoch 18/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1345 - accuracy: 0.8532\n",
      "Epoch 19/100\n",
      "470/470 [==============================] - 0s 73us/sample - loss: 0.1370 - accuracy: 0.8340\n",
      "Epoch 20/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1415 - accuracy: 0.8404\n",
      "Epoch 21/100\n",
      "470/470 [==============================] - 0s 74us/sample - loss: 0.1325 - accuracy: 0.8426\n",
      "Epoch 22/100\n",
      "470/470 [==============================] - 0s 70us/sample - loss: 0.1342 - accuracy: 0.8468\n",
      "Epoch 23/100\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.90 - 0s 79us/sample - loss: 0.1406 - accuracy: 0.8426\n",
      "Epoch 24/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1362 - accuracy: 0.8404\n",
      "Epoch 25/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1405 - accuracy: 0.8298\n",
      "Epoch 26/100\n",
      "470/470 [==============================] - 0s 74us/sample - loss: 0.1352 - accuracy: 0.8426\n",
      "Epoch 27/100\n",
      "470/470 [==============================] - 0s 73us/sample - loss: 0.1399 - accuracy: 0.8362\n",
      "Epoch 28/100\n",
      "470/470 [==============================] - 0s 75us/sample - loss: 0.1313 - accuracy: 0.8553\n",
      "Epoch 29/100\n",
      "470/470 [==============================] - 0s 75us/sample - loss: 0.1349 - accuracy: 0.8489\n",
      "Epoch 30/100\n",
      "470/470 [==============================] - 0s 76us/sample - loss: 0.1377 - accuracy: 0.8383\n",
      "Epoch 31/100\n",
      "470/470 [==============================] - 0s 79us/sample - loss: 0.1391 - accuracy: 0.8362\n",
      "Epoch 32/100\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.1991 - accuracy: 0.80 - 0s 76us/sample - loss: 0.1497 - accuracy: 0.8191\n",
      "Epoch 33/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1332 - accuracy: 0.8532\n",
      "Epoch 34/100\n",
      "470/470 [==============================] - 0s 72us/sample - loss: 0.1364 - accuracy: 0.8511\n",
      "Epoch 35/100\n",
      "470/470 [==============================] - 0s 75us/sample - loss: 0.1359 - accuracy: 0.8383\n",
      "Epoch 36/100\n",
      "470/470 [==============================] - 0s 74us/sample - loss: 0.1351 - accuracy: 0.8489\n",
      "Epoch 37/100\n",
      "470/470 [==============================] - 0s 79us/sample - loss: 0.1341 - accuracy: 0.8298\n",
      "Epoch 38/100\n",
      "470/470 [==============================] - 0s 76us/sample - loss: 0.1353 - accuracy: 0.8383\n",
      "Epoch 39/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1350 - accuracy: 0.8340\n",
      "Epoch 40/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1313 - accuracy: 0.8489\n",
      "Epoch 41/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1360 - accuracy: 0.8234\n",
      "Epoch 42/100\n",
      "470/470 [==============================] - 0s 73us/sample - loss: 0.1319 - accuracy: 0.8489\n",
      "Epoch 43/100\n",
      "470/470 [==============================] - 0s 79us/sample - loss: 0.1353 - accuracy: 0.8468\n",
      "Epoch 44/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1304 - accuracy: 0.8468\n",
      "Epoch 45/100\n",
      "470/470 [==============================] - 0s 71us/sample - loss: 0.1407 - accuracy: 0.8340\n",
      "Epoch 46/100\n",
      "470/470 [==============================] - 0s 73us/sample - loss: 0.1315 - accuracy: 0.8489\n",
      "Epoch 47/100\n",
      "470/470 [==============================] - 0s 80us/sample - loss: 0.1308 - accuracy: 0.8489\n",
      "Epoch 48/100\n",
      "470/470 [==============================] - 0s 79us/sample - loss: 0.1359 - accuracy: 0.8383\n",
      "Epoch 49/100\n",
      "470/470 [==============================] - 0s 75us/sample - loss: 0.1302 - accuracy: 0.8426\n",
      "Epoch 50/100\n",
      "470/470 [==============================] - 0s 75us/sample - loss: 0.1328 - accuracy: 0.8468\n",
      "Epoch 51/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1336 - accuracy: 0.8468\n",
      "Epoch 52/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1345 - accuracy: 0.8447\n",
      "Epoch 53/100\n",
      "470/470 [==============================] - 0s 80us/sample - loss: 0.1330 - accuracy: 0.8362\n",
      "Epoch 54/100\n",
      "470/470 [==============================] - 0s 70us/sample - loss: 0.1326 - accuracy: 0.8447\n",
      "Epoch 55/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1360 - accuracy: 0.8468\n",
      "Epoch 56/100\n",
      "470/470 [==============================] - 0s 73us/sample - loss: 0.1335 - accuracy: 0.8426\n",
      "Epoch 57/100\n",
      "470/470 [==============================] - 0s 71us/sample - loss: 0.1340 - accuracy: 0.8404\n",
      "Epoch 58/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1334 - accuracy: 0.8511\n",
      "Epoch 59/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1348 - accuracy: 0.8404\n",
      "Epoch 60/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1287 - accuracy: 0.8468\n",
      "Epoch 61/100\n",
      "470/470 [==============================] - 0s 79us/sample - loss: 0.1360 - accuracy: 0.8426\n",
      "Epoch 62/100\n",
      "470/470 [==============================] - 0s 74us/sample - loss: 0.1324 - accuracy: 0.8404\n",
      "Epoch 63/100\n",
      "470/470 [==============================] - 0s 75us/sample - loss: 0.1337 - accuracy: 0.8468\n",
      "Epoch 64/100\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.70 - 0s 76us/sample - loss: 0.1293 - accuracy: 0.8468\n",
      "Epoch 65/100\n",
      "470/470 [==============================] - 0s 75us/sample - loss: 0.1305 - accuracy: 0.8426\n",
      "Epoch 66/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1325 - accuracy: 0.8362\n",
      "Epoch 67/100\n",
      "470/470 [==============================] - 0s 73us/sample - loss: 0.1281 - accuracy: 0.8553\n",
      "Epoch 68/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1324 - accuracy: 0.8404\n",
      "Epoch 69/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1323 - accuracy: 0.8362\n",
      "Epoch 70/100\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.90 - 0s 76us/sample - loss: 0.1268 - accuracy: 0.8447\n",
      "Epoch 71/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1284 - accuracy: 0.8426\n",
      "Epoch 72/100\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.0096 - accuracy: 1.00 - 0s 77us/sample - loss: 0.1341 - accuracy: 0.8340\n",
      "Epoch 73/100\n",
      "470/470 [==============================] - 0s 74us/sample - loss: 0.1334 - accuracy: 0.8532\n",
      "Epoch 74/100\n",
      "470/470 [==============================] - 0s 74us/sample - loss: 0.1309 - accuracy: 0.8447\n",
      "Epoch 75/100\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.1603 - accuracy: 0.80 - 0s 77us/sample - loss: 0.1293 - accuracy: 0.8426\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1320 - accuracy: 0.8426\n",
      "Epoch 77/100\n",
      "470/470 [==============================] - 0s 75us/sample - loss: 0.1294 - accuracy: 0.8404\n",
      "Epoch 78/100\n",
      "470/470 [==============================] - 0s 73us/sample - loss: 0.1284 - accuracy: 0.8468\n",
      "Epoch 79/100\n",
      "470/470 [==============================] - 0s 74us/sample - loss: 0.1336 - accuracy: 0.8404\n",
      "Epoch 80/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1314 - accuracy: 0.8468\n",
      "Epoch 81/100\n",
      "470/470 [==============================] - 0s 73us/sample - loss: 0.1286 - accuracy: 0.8489\n",
      "Epoch 82/100\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.70 - 0s 78us/sample - loss: 0.1308 - accuracy: 0.8489\n",
      "Epoch 83/100\n",
      "470/470 [==============================] - 0s 74us/sample - loss: 0.1281 - accuracy: 0.8426\n",
      "Epoch 84/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1277 - accuracy: 0.8404\n",
      "Epoch 85/100\n",
      "470/470 [==============================] - 0s 75us/sample - loss: 0.1291 - accuracy: 0.8426\n",
      "Epoch 86/100\n",
      "470/470 [==============================] - 0s 73us/sample - loss: 0.1366 - accuracy: 0.8362\n",
      "Epoch 87/100\n",
      "470/470 [==============================] - 0s 76us/sample - loss: 0.1302 - accuracy: 0.8447\n",
      "Epoch 88/100\n",
      "470/470 [==============================] - 0s 80us/sample - loss: 0.1270 - accuracy: 0.8553\n",
      "Epoch 89/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1321 - accuracy: 0.8468\n",
      "Epoch 90/100\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.90 - 0s 76us/sample - loss: 0.1284 - accuracy: 0.8489\n",
      "Epoch 91/100\n",
      "470/470 [==============================] - 0s 78us/sample - loss: 0.1275 - accuracy: 0.8489\n",
      "Epoch 92/100\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.2625 - accuracy: 0.60 - 0s 73us/sample - loss: 0.1262 - accuracy: 0.8468\n",
      "Epoch 93/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1303 - accuracy: 0.8404\n",
      "Epoch 94/100\n",
      "470/470 [==============================] - 0s 70us/sample - loss: 0.1273 - accuracy: 0.8404\n",
      "Epoch 95/100\n",
      "470/470 [==============================] - 0s 72us/sample - loss: 0.1265 - accuracy: 0.8511\n",
      "Epoch 96/100\n",
      "470/470 [==============================] - 0s 72us/sample - loss: 0.1287 - accuracy: 0.8511\n",
      "Epoch 97/100\n",
      "470/470 [==============================] - 0s 74us/sample - loss: 0.1303 - accuracy: 0.8426\n",
      "Epoch 98/100\n",
      "470/470 [==============================] - 0s 77us/sample - loss: 0.1227 - accuracy: 0.8553\n",
      "Epoch 99/100\n",
      "470/470 [==============================] - 0s 76us/sample - loss: 0.1386 - accuracy: 0.8298\n",
      "Epoch 100/100\n",
      "470/470 [==============================] - 0s 75us/sample - loss: 0.1297 - accuracy: 0.8468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2241e25ea48>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y,epochs=100,batch_size=10)\n",
    "### 모델을 실제로 수행하는 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0b50c",
   "metadata": {},
   "source": [
    "# +) Cost Function(오차 함수)\n",
    "오차 함수에 따라서 모델의 성능이 달라지기 때문에 적절한 오차 함수를 사용해줘야 한다.   \n",
    "오차 함수의 종류는 평균 제곱 오차 계열과 교차 엔트로피 계열로 나누어진다.  \n",
    "* 평균 제곱 오차 계열: 수렴하기까지 속도가 많이 걸린다.\n",
    "* 교차 엔트로피 계열: 출력 값에 로그를 취하는 방법으로 오차가 커지면 수렴 속도가 빨리지고, 오차가 작아지면 수렴 속도가 감소하도록 한다. 주로 분류 문제에 많이 사용된다.\n",
    "\n",
    "| 평균 제곱 계열 | mean_squared_error | 평균 제곱 오차 |\n",
    "| --- | --- | --- | \n",
    "| | mean_absolute_error |  평균 절대 오차 | \n",
    "| | mean_absolute_percentage_error | 평균 절대 백분율 오차 |\n",
    "| | mean_squared_logarithmic_error | 평균 제곱 로그 오차 |\n",
    "| 교차 엔트로피 계열 | categorical_crossentropy | 범주형 교차 엔트로피(일반적인 분류) |\n",
    "|  | binary_crossentropy | 이항 교차 엔트로피(두 개의 클래스 중에서 예측할 때) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed45bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
