# 모델 성능을 높여보자!🎈
`좋은 인공지능 모델`이란 무엇일까요? <br>
당연히 train set을 벗어난 **완전히 새로운 데이터의 결과도 알맞게 예측할 수 있는 모델**일 것입니다. 그러기 위해서 개발자들은 아래의 두 가지를 고려하면서 모델의 예측 정확도를 높일 수 있어야 합니다.<br>
1. 모델이 기존 데이터에 `underfitting/overfitting` 되지 않아야 한다.
2. 모델이 새로운 데이터에 유연히 대처할 수 있도록 충분히 `일반화(generalization)` 되어야 한다. <br>

사실 이 둘은 어찌보면 같은 맥락 상에 있습니다. 그렇다면 모델의 성능을 높이기 위해 어떤 것들을 시도해볼 수 있을까요? <br>

1. `데이터 조작`
2. `알고리즘 튜닝` <br>

크게 두 가지 방향으로 모델 성능을 개선할 수 있습니다. 일반적으로는 `데이터 조작`이 모델 성능에 가장 큰 영향을 주고 번호 순으로 그 영향이 작아집니다. <br>
<br>
[How To Improve Deep Learning Performance](https://machinelearningmastery.com/improve-deep-learning-performance/) 포스트를 기반으로 , `AI study 수업 시간에 다루어 본 방법들`을 추려 소개해보려 합니다. <br>
<br>
## 목차
1. 데이터 조작
     - 데이터 스케일링
     - validation 데이터 교차 검증화(Cross-validation)
     - 튀는 데이터 살펴보기
2. 알고리즘 튜닝
     - 학습률(Learning Rate)
     - 활성화 함수(Activation Functions)
     - 규제(Regularization)
     - 오차 함수(Loss function)
     -  옵티마이저(Opimizatizer)
     -  Early Stopping



그럼 차례대로 각각 어떤 방법을 사용해볼 수 있나 살펴보겠습니다.

## Ⅰ. 데이터 조작
### 🔻 데이터 스케일링
모델에 데이터를 넣을 때, feature 마다 데이터의 범위가 제각각이라면 feature 마다 결과 값에 행사하는 영향력이 달라지기 때문에 모델이 예측을 제대로 하는 것이 불가능합니다. 이런 이유로 **데이터를 모델에 넣기 전에 스케일링하는 과정**이 필요합니다. <br>
`스케일링(scaling)`이란 **모델에 사용할 데이터의 feature들의 범위를 통일시키는 작업**을 말합니다. feature 마다 다른 데이터의 단위를 서로 비교 가능하게 변환해주는 것입니다. 데이터를 스케일링하는 데 사용되는 개념은 주로 두 가지입니다. 바로 `Standardization(표준화)`와 `Normalization(정규화)`입니다.
- `Standardization(표준화)`: feature 마다 갖고 있는 데이터의 평균을 0, 분산을 1로 바꿉니다. 즉, 데이터를 정규분포로 만듭니다.
- `Normalization(정규화)`: feature 마다 갖고 있는 데이터의 범위가 한정되도록 바꿉니다. ex. [0,1] (0에서 1 사이의 값만 갖게 됨) <br>
![image](https://user-images.githubusercontent.com/78032658/152022273-75363d5d-46e3-4d8f-9971-59dd60e86691.png)

#### ✔ 예시
`scikit-learn`에서 여러가지 스케일링 함수를 제공하는데, 데이터의 상황에 따라 적절한 `스케일러`를 도입해보며 성능을 향상시켜 봅시다.
- `StandardScaler()`: 데이터를 정규분포로 만든다. 분류에 주로 사용.
- `MinMaxScalar()`: 데이터를 특정 범위([0,1])로 만든다. 회귀에 주로 사용.
- `MaxAbsScaler()`: 데이터를 특정 범위([-1,1])로 만든다. 
- `RobustScaler()`: 평균과 분산 대신에 중간 값과 사분위 값을 사용하여 튀는 데이터(노이즈)에 의한 오차를 개선할 수 있다.
- `Normalizer()` <br>


#### ✔ 참고
- [데이터 스케일리의 개념과 5가지 scikit-learn scaler 함수](https://wooono.tistory.com/96) <br>
<br>

### 🔻 validation 데이터 교차 검증화(Cross-validation)
훈련한 모델의 상태를 판단하기 위한 데이터인 `validation data`는 모델의 성능을 검진할 수 있는 귀한 자료입니다. <br>
`validation data`를 충분히 확보하면 그만큼 모델의 상태도 판단할 수 있기 때문에 성능 개선에 도움이 됩니다. <br>
하지만 `validation data`를 늘리면 `train data`는 줄어들게 됩니다. 이러한 문제를 해결하기 위해 `교차검증(cross-validation)`을 사용합니다.<br>
#### ✔ 예시
- `K겹 교차검증(K-fold Cross-validation)`


### 🔻 튀는 데이터 살펴보기
데이터의 경향성에 어긋나는 데이터를 흔히 튀는 데이터, 즉 `이상치(outlier, 노이즈)`라고 표현합니다. <br>
데이터에 `노이즈`가 있나 살펴보고 조작하면 모델의 성능을 개선할 수 있습니다. `데이터 시각화`를 하면 `노이즈`를 시각 자료로 확인할 수 있습니다. <br>
<br>


## Ⅱ. 알고리즘 튜닝
환자가 아파서 병원에 가면 의사 선생님이 무턱대로 약을 처방해주지는 않습니다. 그 전에 `진단`이 필요하죠. <br>
마찬가지로 골골대는 우리의 모델을 치료해주기 위해서는 그 원인을 `진단`해야 합니다. 즉, 모델의 `성능 값을 모니터링`하고 이를 기반으로 상태를 `분석`할 수 있어야 합니다. <br>
<br>
이때 `train set`의 정확도와 `test(val) set`의 정확도를 모니터링 및 분석하여 `underfitting` 상태인지 `overfitting` 상태인지 판단할 수 있습니다.<br>
  - `train set`과 `test(val) set` 에서 모두 정확도가 낮다. -> `underfitting`
  - `train set`에 비해 `test(val) set`에서 정확도가 낮다. -> `overfitting` <br>
  ![image](https://user-images.githubusercontent.com/78032658/152030055-c8304471-313f-4cc6-8864-4bec9cffb37b.png)


현재 모델의 상태 분석을 완료했다면 그 문제를 해결하기 위한 방법들을 알아봅시다.<br>
<br>
### 🔻 학습률(Learning Rate)
여러 번 모델을 돌려보고 사용 중인 데이터/파라미터/환경 등에 적절한 `learning rate`를 찾아야 합니다. <br>
![image](https://user-images.githubusercontent.com/78032658/152038518-4b805314-a876-46d2-b3d0-5b8ceba3bb5a.png)


### 🔻 활성화 함수(Activation Functions)
적절한 `activation function`을 사용하고 있는지 점검해봅니다. <br>
딥러닝 출력 값의 형태나 딥러닝의 특성 따라 다른 `activation function`을 사용해야 하기 때문에 모델의 각 layer 상황에 맞는 `activation function`을 설정해줘야 합니다.
#### ✔ 예시

- `Sigmoid`
- `ReLU`
- `Softmax` <br>


#### ✔ 참고
[활성화 함수(activation function)종류와 장단점에 따른 쓰임 정리](https://ganghee-lee.tistory.com/32) <br>


### 🔻 규제(Regularization)
`Regularization`은 `overfitting` 상태의 모델의 성능을 개선시키기 위해 주로 사용됩니다.<br>
`overfitting`이 발생하는 이유는 주로 `train set`의 `노이즈`에 까지 과하게 `fit`되기 때문이죠. 이때 `regularization`를 적용하면, 단순히 `오차`가 적어지는 방향으로만 학습을 진행하지 않고 `오차와 가중치` 모두가 최소가 되는 방향으로 학습을 진행합니다.<br>
따라서 `노이즈`의 영향을 줄이고 `일반화(generalization)`하여 `overfitting`을 막을 수 있습니다.
#### ✔ 예시

- `Dropout`
- `L1`
- `L2` <br>

![image](https://user-images.githubusercontent.com/78032658/152031297-781f3e3c-4ff4-4c0b-95dc-860e7522afb2.png)


### 🔻 오차 함수(Loss function)
모델을 `회귀` 목적으로 사용할 건지 `분류` 목적으로 사용할 건지에 따라 다른 `오차 함수`를 사용합니다. <br>
#### ✔ 예시

- `평균 제곱 오차(MSE)`: for `회귀`
- `크로스 엔트로피(Cross-Entropy)`: for `분류`
  - `이진 크로스 엔트로피(Binary Cross-Entropy)`: for `이진 분류`
  - `범주형 크로스 엔트로피(Categorical Cross-Entropy)`: for `다중 분류` <br>


### 🔻ⅴ. 옵티마이저(Opimizatizer)
모델은 `옵티마이저`를 통해 `오차(loss function)`를 최소화하는 `파라미터`(가중치, 편향)를 찾습니다. <br>
그리고 어떤 `옵티마이저`를 사용하냐에 따라 `오차`(loss function)를 최소화하는 `알고리즘`이 달라집니다.<br>
#### ✔ 예시

- `확률적 경사 하강법(SGD)`
- `모멘텀(Momentum)`
- `아다그라드(Adagrad)`
- `아담(Adam)` <br>
![image](https://user-images.githubusercontent.com/78032658/152046113-0df563d7-fcae-4da5-a186-13bde9e88e14.png)


### 🔻 Early Stopping
`overfitting`을 방지하기 위해서 `Early Stopping`을 설정할 수 있습니다.<br>
`checkpoint`도 저장할 수 있기 때문에 모델의 상태를 살피고 성능을 개선하는 데에 좋습니다.<br>
<br>

<hr>
<br>

## Plus!✌ 파라미터(Parameter) vs. 하이퍼 파라미터(Hyper parameter)
`파라미터(Parameter)`와 `하이퍼 파라미터(Hyper parameter)`는 다른 개념입니다.
- `파라미터(Parameter)`
  - 모델이 학습되며 모델 내부에서 결정되는 변수
  - 가중치(weight)와 편향(bias) 등이 해당
- `하이퍼 파라미터(Hyper parameter)`
  - 모델링할 때 사용자가 직접 값을 설정해주는 변수
  - 신경망의 넓이와 깊이, 학습률(learning rate), 에포크(epoch), iteration 등이 해당
  - 정해진 최적의 값이 없기 때문에 경험으로 결정된다. 이를 선택하기 위해 validation set이 사용된다.
  - 자동으로 하이퍼 파라미터를 정해주는 라이브러리 사용할 수 있다. ex. 베이지안 옵티미제이션 <br>

[하이퍼파라미터 튜닝 팁](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)
